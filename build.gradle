import org.apache.tools.ant.taskdefs.condition.Os

plugins {
    id("base")
}

buildDir = "${projectDir}/.output"

private static String buildTimestamp() {
    return new Date().format('yyyyMMddHHmmss')
}

private static String dockerImage() {
    String fileContents = new File('./Dockerimage').text
    return fileContents
}

private static Iterable<String> osAdaptiveCommand(String... commands) {
    def newCommands = []
    if (Os.isFamily(Os.FAMILY_WINDOWS)) {
        newCommands = ['cmd', '/c']
    }
    newCommands.addAll(commands)
    newCommands.addAll([])
    return newCommands
}

private static Boolean useDocker() {
    return System.getenv("SPARK_HOME") == null
}

private Iterable<String> dbtCommand(String... commands) {
    def newCommands = []
    def addlArgs = []
    if (useDocker()) {
        newCommands = [
            "docker", "run", "--rm",
            "-e", "SPARK_WAREHOUSE_DIR=/spark_warehouse",
            "-v", "warehouse:/spark_warehouse",
            // "-v", "C:\\Files\\Source\\dataops-tools:/opt/conda/lib/python3.7/site-packages/slalom.dataops-1.0.0.dev0-py3.7.egg",
            "-p", "10000:10000",
            "-w", "/projects/my-project",
            "-v", "${projectDir}:/projects/my-project",
            "-v", "${projectDir}/.devcontainer/.bashhist:/root/.bash_history",
            "-e", "AWS_CONFIG_FILE=/projects/my-project/.secrets/aws_credentials",
            "-e", "WITH_SPARK=true",
            "-e", "AWS_SHARED_CREDENTIALS_FILE=/projects/my-project/.secrets/aws_credentials",
            dockerImage(),
            "dbt-spark"
        ]
    }
    else {
        newCommands = ["dbt-spark"]
    }
    newCommands.addAll(commands)
    addlArgs = ["--profiles-dir", "./tools/dbt/config"]
    newCommands.addAll(addlArgs)
    return newCommands
}

private Iterable<String> dbtCommandWith8081(String... commands) {
    def newCommands = []
    def addlArgs = []
    newCommands = [
        "docker", "run", "--rm",
        "-e", "PROJECT_NAME=dataops-project",
        "-v", "${projectDir}:/projects/dataops-project",
        "-p", "8081:8081",
        "-w", "/projects/dataops-project/transform",
        "-v", "${projectDir}/.devcontainer/.bashhist:/root/.bash_history",
        dockerImage(),
        "dbt-spark"
    ]
    newCommands.addAll(commands)
    addlArgs = ["--profiles-dir", "./tools/dbt/config"]
    newCommands.addAll(addlArgs)
    return newCommands
}

task zip(type: Zip, description: "Archive project definition as a zip") {
    from("data")
    setArchiveFileName("project.zip")
}

task dockerDestroyContainers(type:Exec) {
    workingDir  = '.'
    commandLine = osAdaptiveCommand(
        "docker-compose",
        "--project-name", "dataops-project-template_devcontainer",
        "-f", ".devcontainer/docker-compose.dev.yml",
        "down", "--remove-orphans"
    )
}

task dockerDestroyVolume(type:Exec) {
    commandLine = osAdaptiveCommand(
        "docker", "volume", "rm", "warehouse",
    )
}

task dockerVolumeLs(type:Exec) {
    commandLine = osAdaptiveCommand(
        "docker", "run", "--rm",
        "-v", "warehouse:/spark_warehouse",
        "busybox", "ls", "/spark_warehouse", "-la",
    )
    doFirst {
        String filePath = "${buildDir}/spark/spark-dw-${buildTimestamp()}-list.txt"
        mkdir("${buildDir}/spark")
        standardOutput = new FileOutputStream(filePath)
    }
    // doLast {
    //     String filePath = "${buildDir}/spark/spark-dw-${buildTimestamp}-list.txt"
    //     String fileContents = new File(filePath).getText('UTF-8')
    //     println(fileContents)
    // }
}

task dockerVolumeExport(type:Exec) {
    dependsOn(dockerVolumeLs)
    commandLine = osAdaptiveCommand(
        "docker", "run", "--rm",
        "-v", "warehouse:/spark_warehouse",
        "-w", "/",
        "busybox", "tar", "-cOzf", "-", "spark_warehouse",
    )
    doFirst {
        mkdir("${buildDir}/spark")
        standardOutput = new FileOutputStream("${buildDir}/spark/spark-dw-${buildTimestamp()}-bak.tgz")
    }
}

task dockerDestroyAll() {
    dependsOn(dockerDestroyContainers, dockerDestroyVolume)
}

task dockerBuild(type:Exec) {
    mustRunAfter(dockerDestroyAll, dockerDestroyContainers, dockerDestroyVolume)
    workingDir  = '.'
    commandLine = osAdaptiveCommand(
        "docker", "build", "-t", dockerImage(), ".",
    )
}

task dockerPush(type:Exec) {
    mustRunAfter(dockerBuild)
    workingDir  = "."
    commandLine = osAdaptiveCommand(
        "docker", "push", dockerImage()
    )
}

task dockerPull(type:Exec) {
    workingDir  = "."
    commandLine = osAdaptiveCommand(
        "docker", "pull", dockerImage()
    )
}

task dockerCompose(type:Exec) {
    dependsOn(dockerBuild)
    mustRunAfter(dockerDestroyAll, dockerDestroyContainers, dockerDestroyVolume)
    workingDir  = '.'
    commandLine = osAdaptiveCommand(
        "docker-compose",
        "--project-name", "dataops-project-template_devcontainer",
        "-f", ".devcontainer/docker-compose.dev.yml",
        "build"
    )
}

task dockerPullGrandparent(type:Exec) {
    // Use only when simultaneously testing changes to the the `dataops-tools` repo locally
    workingDir  = '../dataops-tools'
    commandLine = osAdaptiveCommand(
        "docker", "pull", "slalomggp/dbt:latest-dev"
    )
}

task dockerBuildParent(type:Exec) {
    // Use only when simultaneously testing changes to the the `dataops-tools` repo locally
    mustRunAfter(dockerPullGrandparent)
    workingDir  = '../dataops-tools'
    commandLine = osAdaptiveCommand(
        "docker", "build", "-t", "slalomggp/dataops:latest-dev", "."
    )
}

task dockerComposeFresh() {
    dependsOn(dockerDestroyAll, dockerCompose)
    mustRunAfter(dockerBuildParent)
}

task dockerUp(type:Exec) {
    dependsOn(dockerCompose)
    workingDir  = '.'
    commandLine = osAdaptiveCommand(
        "docker-compose",
        "--project-name", "dataops-project-template_devcontainer",
        "-f", ".devcontainer/docker-compose.dev.yml",
        "up"
    )
}

task dockerFreshUp() {
    dependsOn(dockerComposeFresh, dockerUp)
}

task dbtCompile(type:Exec) {
    workingDir  = '.'
    commandLine = dbtCommand("compile")
    doFirst {
        mkdir ".output/dbt"
    }
}

task dbtSeed(type:Exec) {
    mustRunAfter(dbtCompile)
    workingDir  = '.'
    commandLine = dbtCommand("seed", "--show")
}

task dbtRun(type:Exec) {
    dependsOn(dbtCompile)
    mustRunAfter(dbtCompile, dbtSeed)
    workingDir  = '.'
    commandLine = dbtCommand("run")
}

task dbtTest(type:Exec) {
    mustRunAfter(dbtSeed, dbtRun, dbtCompile)
    workingDir  = '.'
    commandLine = dbtCommand("test")
}

task dbtHelp(type:Exec) {
    workingDir  = '.'
    commandLine = dbtCommand("--help")
}

task dbtDocs(type:Exec) {
    workingDir  = '.'
    commandLine = dbtCommand("docs", "generate")
}

task dbtDocsServe(type:Exec) {
    dependsOn(dbtDocs)
    workingDir  = '.'
    commandLine = dbtCommandWith8081("docs", "serve", "--port", "8081")
}

task tapPlan(type:Exec) {
    workingDir  = 'data/taps'
    commandLine = osAdaptiveCommand(
        "./plan.sh"
    )
}

task tapSync(type:Exec) {
    mustRunAfter(tapPlan)
    workingDir  = 'data/taps'
    commandLine = osAdaptiveCommand(
        "./sync.sh"
    )
}

task compile() {
    dependsOn(dbtCompile, dbtDocs)
}

task load() {
    dependsOn(dbtSeed)
    mustRunAfter(compile)
}

task transform() {
    dependsOn(dbtRun)
    mustRunAfter(compile, load)
}

task test() {
    dependsOn(dbtTest)
    mustRunAfter(compile, load, transform)
}

task runAll() {
    dependsOn(compile, load, transform, test)
}

task rebuildUp(type:Exec) {
}

task testUDFs(type:Exec) {
    mustRunAfter(dockerCompose, dockerBuildParent, dockerPullGrandparent)
    // dependsOn(dockerCompose)
    workingDir  = "."
    commandLine = dbtCommand("run")
}
